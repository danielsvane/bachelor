\relax 
\providecommand{\transparent@use}[1]{}
\providecommand \oddpage@label [2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Neural networks}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}In silico neural networks}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Diagram of the artificial perceptron.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{perceptron}{{1}{1}}
\citation{Qian}
\citation{Qian2011}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax }}{2}}
\newlabel{perceptron_simple}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}In vitro perceptron}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The back on forth reactions of the seesaw gate.\relax }}{2}}
\newlabel{seesaw}{{3}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Thresholding}{3}}
\newlabel{}{{4a}{3}}
\newlabel{sub@}{{a}{3}}
\newlabel{}{{4b}{3}}
\newlabel{sub@}{{b}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Integration}{3}}
\newlabel{}{{5a}{4}}
\newlabel{sub@}{{a}{4}}
\newlabel{}{{5b}{4}}
\newlabel{sub@}{{b}{4}}
\newlabel{seesaw_integration}{{\caption@xref {seesaw_integration}{ on input line 101}}{4}}
\@writefile{toc}{\contentsline {subsubsection}{Weighting}{4}}
\newlabel{}{{6a}{5}}
\newlabel{sub@}{{a}{5}}
\newlabel{}{{6b}{5}}
\newlabel{sub@}{{b}{5}}
\newlabel{seesaw_weighting}{{\caption@xref {seesaw_weighting}{ on input line 122}}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Neuron}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Schematic of a 2-input neuron implemented with seesaw gates. The neuron has an input gate for each input, which makes sure that the input is higher than a give threshold before the input is registered. The gate and fuel concentrations in each input gate affects the weight of the input before it is sent to the integration gate. The integration gate collects the right recognition sequence from the input gates for the threshold gate. The threshold gate activates if the sum of the weighted inputs is larger than the threshold concentration of the treshold gate.\relax }}{6}}
\newlabel{seesaw_neuron}{{7}{6}}
\citation{Qian2011}
\citation{Qian2011}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The strands and concentrations of the units of a seesaw gate can be represented schematically to simplify larger diagrams.\relax }}{7}}
\newlabel{seesaw_gate_simple}{{8}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The seesaw neuron from Figure~\ref  {seesaw_neuron} shown schematically.\relax }}{7}}
\newlabel{seesaw_neuron_schematic}{{9}{7}}
\citation{Picuri2009}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Truth table for an AND gate.\relax }}{8}}
\newlabel{and_table}{{1}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Training}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Input translation}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Translation of arbitrary input sequences into the syntax required for the seesaw neural network. The input strand $ab$ displaces $bS_1T$ from the first half-translator. $bS_1T$ can then displace $S_1TS_2$ from the second half translator. This process successfully translates the input $ab$ into $S_1TS_2$, which can then be used for input in a neural network (see Figure~\ref  {seesaw_neuron}).\relax }}{9}}
\newlabel{translator}{{10}{9}}
